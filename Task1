import spacy

# Load the English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Sample text
text = ("Is Rome the capital of Italy? Surely it is, but many donâ€™t know this fact that Italy was not always "
        "called as Italy. Before Italy came into being in 1861, it had several names including Italian Kingdom, "
        "Roman Empire and the Republic of Italy among others.")

# Process the text
doc = nlp(text)

# Extract entities and remove any duplicates or unnecessary punctuation
entities = list(set((ent.text.replace(',', '').strip(), ent.label_) for ent in doc.ents))


# Function to correct entity names for better URL generation
def correct_entity_name(entity):
    corrections = {
        'the Republic of Italy': 'Kingdom_of_Italy',  # Using historical name for clarity
        'Italian Kingdom': 'Kingdom_of_Italy'  # Direct to the correct historical page
    }
    return corrections.get(entity, entity)

# Function to generate Wikipedia URL for an entity
def entity_to_wikipedia_url(entity):
    corrected_entity = correct_entity_name(entity)
    wikipedia_suffix = corrected_entity.replace(' ', '_')
    return f"https://en.wikipedia.org/wiki/{wikipedia_suffix}"

# Print identified entities and their Wikipedia URLs
print("Corrected entities and Wikipedia URLs:")
for entity, label in entities:
    if label == 'GPE':  # We focus only on geopolitical entities
        url = entity_to_wikipedia_url(entity)
        print(f"{entity} => {url}")
